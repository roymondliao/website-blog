---
title: "Attention Is All You Need"
date: 2019-11-09
lastmod: 2019-11-09
draft: true
authors: ["Roymond Liao"]
categories:
    - NLP
    - Deep Learning
tags: ["Attention", "Self-attention", "Transformer"]
markup: mmark
image:
  placement: 2
  caption: ""
  focal_point: ""
  preview_only: false
---

## Refenece

Illustrate:

1. [The IIIustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
2. [w淺談神經機器翻譯 & 用 Transformer 與 Tensorflow2](https://leemeng.tw/neural-machine-translation-with-transformer-and-tensorflow2.html#top)
3. [Attention is all you need 解讀](https://zhuanlan.zhihu.com/p/34781297)
4. [Transformer model for language understanding by google](https://www.tensorflow.org/tutorials/text/transformer)
5. [How Self-Attention with Relative Position Representations works](https://medium.com/@_init_/how-self-attention-with-relative-position-representations-works-28173b8c245a)

Tutorial:

1. [Neural Machine Translation (seq2seq) Tutorial](https://github.com/tensorflow/nmt#background-on-the-attention-mechanism)
2. https://www.tensorflow.org/tutorials/text/transformer
3. [Guide annotating the paper with PyTorch implementation](http://nlp.seas.harvard.edu/2018/04/03/attention.html)

Visualization:

1. https://github.com/jessevig/bertviz

